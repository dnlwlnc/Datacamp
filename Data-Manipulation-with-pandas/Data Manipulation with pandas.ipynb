{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0              region       state  individuals  family_members  \\\n",
      "0           0  East South Central     Alabama       2570.0           864.0   \n",
      "1           1             Pacific      Alaska       1434.0           582.0   \n",
      "2           2            Mountain     Arizona       7259.0          2606.0   \n",
      "3           3  West South Central    Arkansas       2280.0           432.0   \n",
      "4           4             Pacific  California     109008.0         20964.0   \n",
      "\n",
      "   state_pop  \n",
      "0    4887681  \n",
      "1     735139  \n",
      "2    7158024  \n",
      "3    3009733  \n",
      "4   39461588  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "homelessness = pd.read_csv('data/homelessness.csv') \n",
    "print(homelessness.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51 entries, 0 to 50\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Unnamed: 0      51 non-null     int64  \n",
      " 1   region          51 non-null     object \n",
      " 2   state           51 non-null     object \n",
      " 3   individuals     51 non-null     float64\n",
      " 4   family_members  51 non-null     float64\n",
      " 5   state_pop       51 non-null     int64  \n",
      "dtypes: float64(2), int64(2), object(2)\n",
      "memory usage: 2.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(homelessness.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 6)\n"
     ]
    }
   ],
   "source": [
    "print(homelessness.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0    individuals  family_members     state_pop\n",
      "count   51.000000      51.000000       51.000000  5.100000e+01\n",
      "mean    25.000000    7225.784314     3504.882353  6.405637e+06\n",
      "std     14.866069   15991.025083     7805.411811  7.327258e+06\n",
      "min      0.000000     434.000000       75.000000  5.776010e+05\n",
      "25%     12.500000    1446.500000      592.000000  1.777414e+06\n",
      "50%     25.000000    3082.000000     1482.000000  4.461153e+06\n",
      "75%     37.500000    6781.500000     3196.000000  7.340946e+06\n",
      "max     50.000000  109008.000000    52070.000000  3.946159e+07\n"
     ]
    }
   ],
   "source": [
    "print(homelessness.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 'East South Central' 'Alabama' 2570.0 864.0 4887681]\n",
      " [1 'Pacific' 'Alaska' 1434.0 582.0 735139]\n",
      " [2 'Mountain' 'Arizona' 7259.0 2606.0 7158024]\n",
      " [3 'West South Central' 'Arkansas' 2280.0 432.0 3009733]\n",
      " [4 'Pacific' 'California' 109008.0 20964.0 39461588]\n",
      " [5 'Mountain' 'Colorado' 7607.0 3250.0 5691287]\n",
      " [6 'New England' 'Connecticut' 2280.0 1696.0 3571520]\n",
      " [7 'South Atlantic' 'Delaware' 708.0 374.0 965479]\n",
      " [8 'South Atlantic' 'District of Columbia' 3770.0 3134.0 701547]\n",
      " [9 'South Atlantic' 'Florida' 21443.0 9587.0 21244317]\n",
      " [10 'South Atlantic' 'Georgia' 6943.0 2556.0 10511131]\n",
      " [11 'Pacific' 'Hawaii' 4131.0 2399.0 1420593]\n",
      " [12 'Mountain' 'Idaho' 1297.0 715.0 1750536]\n",
      " [13 'East North Central' 'Illinois' 6752.0 3891.0 12723071]\n",
      " [14 'East North Central' 'Indiana' 3776.0 1482.0 6695497]\n",
      " [15 'West North Central' 'Iowa' 1711.0 1038.0 3148618]\n",
      " [16 'West North Central' 'Kansas' 1443.0 773.0 2911359]\n",
      " [17 'East South Central' 'Kentucky' 2735.0 953.0 4461153]\n",
      " [18 'West South Central' 'Louisiana' 2540.0 519.0 4659690]\n",
      " [19 'New England' 'Maine' 1450.0 1066.0 1339057]\n",
      " [20 'South Atlantic' 'Maryland' 4914.0 2230.0 6035802]\n",
      " [21 'New England' 'Massachusetts' 6811.0 13257.0 6882635]\n",
      " [22 'East North Central' 'Michigan' 5209.0 3142.0 9984072]\n",
      " [23 'West North Central' 'Minnesota' 3993.0 3250.0 5606249]\n",
      " [24 'East South Central' 'Mississippi' 1024.0 328.0 2981020]\n",
      " [25 'West North Central' 'Missouri' 3776.0 2107.0 6121623]\n",
      " [26 'Mountain' 'Montana' 983.0 422.0 1060665]\n",
      " [27 'West North Central' 'Nebraska' 1745.0 676.0 1925614]\n",
      " [28 'Mountain' 'Nevada' 7058.0 486.0 3027341]\n",
      " [29 'New England' 'New Hampshire' 835.0 615.0 1353465]\n",
      " [30 'Mid-Atlantic' 'New Jersey' 6048.0 3350.0 8886025]\n",
      " [31 'Mountain' 'New Mexico' 1949.0 602.0 2092741]\n",
      " [32 'Mid-Atlantic' 'New York' 39827.0 52070.0 19530351]\n",
      " [33 'South Atlantic' 'North Carolina' 6451.0 2817.0 10381615]\n",
      " [34 'West North Central' 'North Dakota' 467.0 75.0 758080]\n",
      " [35 'East North Central' 'Ohio' 6929.0 3320.0 11676341]\n",
      " [36 'West South Central' 'Oklahoma' 2823.0 1048.0 3940235]\n",
      " [37 'Pacific' 'Oregon' 11139.0 3337.0 4181886]\n",
      " [38 'Mid-Atlantic' 'Pennsylvania' 8163.0 5349.0 12800922]\n",
      " [39 'New England' 'Rhode Island' 747.0 354.0 1058287]\n",
      " [40 'South Atlantic' 'South Carolina' 3082.0 851.0 5084156]\n",
      " [41 'West North Central' 'South Dakota' 836.0 323.0 878698]\n",
      " [42 'East South Central' 'Tennessee' 6139.0 1744.0 6771631]\n",
      " [43 'West South Central' 'Texas' 19199.0 6111.0 28628666]\n",
      " [44 'Mountain' 'Utah' 1904.0 972.0 3153550]\n",
      " [45 'New England' 'Vermont' 780.0 511.0 624358]\n",
      " [46 'South Atlantic' 'Virginia' 3928.0 2047.0 8501286]\n",
      " [47 'Pacific' 'Washington' 16424.0 5880.0 7523869]\n",
      " [48 'South Atlantic' 'West Virginia' 1021.0 222.0 1804291]\n",
      " [49 'East North Central' 'Wisconsin' 2740.0 2167.0 5807406]\n",
      " [50 'Mountain' 'Wyoming' 434.0 205.0 577601]]\n",
      "Index(['Unnamed: 0', 'region', 'state', 'individuals', 'family_members',\n",
      "       'state_pop'],\n",
      "      dtype='object')\n",
      "RangeIndex(start=0, stop=51, step=1)\n"
     ]
    }
   ],
   "source": [
    "# Print the values of homelessness\n",
    "print(homelessness.values)\n",
    "\n",
    "# Print the column index of homelessness\n",
    "print(homelessness.columns)\n",
    "\n",
    "# Print the row index of homelessness\n",
    "print(homelessness.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0              region         state  individuals  family_members  \\\n",
      "50          50            Mountain       Wyoming        434.0           205.0   \n",
      "34          34  West North Central  North Dakota        467.0            75.0   \n",
      "7            7      South Atlantic      Delaware        708.0           374.0   \n",
      "39          39         New England  Rhode Island        747.0           354.0   \n",
      "45          45         New England       Vermont        780.0           511.0   \n",
      "\n",
      "    state_pop  \n",
      "50     577601  \n",
      "34     758080  \n",
      "7      965479  \n",
      "39    1058287  \n",
      "45     624358  \n"
     ]
    }
   ],
   "source": [
    "homelessness_ind = homelessness.sort_values('individuals')\n",
    "print(homelessness_ind.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0              region          state  individuals  \\\n",
      "32          32        Mid-Atlantic       New York      39827.0   \n",
      "4            4             Pacific     California     109008.0   \n",
      "21          21         New England  Massachusetts       6811.0   \n",
      "9            9      South Atlantic        Florida      21443.0   \n",
      "43          43  West South Central          Texas      19199.0   \n",
      "\n",
      "    family_members  state_pop  \n",
      "32         52070.0   19530351  \n",
      "4          20964.0   39461588  \n",
      "21         13257.0    6882635  \n",
      "9           9587.0   21244317  \n",
      "43          6111.0   28628666  \n"
     ]
    }
   ],
   "source": [
    "homelessness_fam = homelessness.sort_values('family_members', ascending=False)\n",
    "print(homelessness_fam.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0              region      state  individuals  family_members  \\\n",
      "13          13  East North Central   Illinois       6752.0          3891.0   \n",
      "35          35  East North Central       Ohio       6929.0          3320.0   \n",
      "22          22  East North Central   Michigan       5209.0          3142.0   \n",
      "49          49  East North Central  Wisconsin       2740.0          2167.0   \n",
      "14          14  East North Central    Indiana       3776.0          1482.0   \n",
      "\n",
      "    state_pop  \n",
      "13   12723071  \n",
      "35   11676341  \n",
      "22    9984072  \n",
      "49    5807406  \n",
      "14    6695497  \n"
     ]
    }
   ],
   "source": [
    "homelessness_reg_fam = homelessness.sort_values(['region', 'family_members'], ascending=[True, False])\n",
    "print(homelessness_reg_fam.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      2570.0\n",
      "1      1434.0\n",
      "2      7259.0\n",
      "3      2280.0\n",
      "4    109008.0\n",
      "Name: individuals, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "individuals = homelessness['individuals']\n",
    "print(individuals.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        state  family_members\n",
      "0     Alabama           864.0\n",
      "1      Alaska           582.0\n",
      "2     Arizona          2606.0\n",
      "3    Arkansas           432.0\n",
      "4  California         20964.0\n"
     ]
    }
   ],
   "source": [
    "state_fam = homelessness[['state', 'family_members']]\n",
    "print(state_fam.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   individuals       state\n",
      "0       2570.0     Alabama\n",
      "1       1434.0      Alaska\n",
      "2       7259.0     Arizona\n",
      "3       2280.0    Arkansas\n",
      "4     109008.0  California\n"
     ]
    }
   ],
   "source": [
    "ind_state = homelessness[['individuals', 'state']]\n",
    "print(ind_state.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0              region       state  individuals  family_members  \\\n",
      "4            4             Pacific  California     109008.0         20964.0   \n",
      "9            9      South Atlantic     Florida      21443.0          9587.0   \n",
      "32          32        Mid-Atlantic    New York      39827.0         52070.0   \n",
      "37          37             Pacific      Oregon      11139.0          3337.0   \n",
      "43          43  West South Central       Texas      19199.0          6111.0   \n",
      "47          47             Pacific  Washington      16424.0          5880.0   \n",
      "\n",
      "    state_pop  \n",
      "4    39461588  \n",
      "9    21244317  \n",
      "32   19530351  \n",
      "37    4181886  \n",
      "43   28628666  \n",
      "47    7523869  \n"
     ]
    }
   ],
   "source": [
    "ind_gt_10k = homelessness[homelessness['individuals'] > 10000]\n",
    "print(ind_gt_10k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0    region       state  individuals  family_members  state_pop\n",
      "2            2  Mountain     Arizona       7259.0          2606.0    7158024\n",
      "5            5  Mountain    Colorado       7607.0          3250.0    5691287\n",
      "12          12  Mountain       Idaho       1297.0           715.0    1750536\n",
      "26          26  Mountain     Montana        983.0           422.0    1060665\n",
      "28          28  Mountain      Nevada       7058.0           486.0    3027341\n",
      "31          31  Mountain  New Mexico       1949.0           602.0    2092741\n",
      "44          44  Mountain        Utah       1904.0           972.0    3153550\n",
      "50          50  Mountain     Wyoming        434.0           205.0     577601\n"
     ]
    }
   ],
   "source": [
    "mountain_reg = homelessness[homelessness['region'] == 'Mountain']\n",
    "print(mountain_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   region   state  individuals  family_members  state_pop\n",
      "1           1  Pacific  Alaska       1434.0           582.0     735139\n"
     ]
    }
   ],
   "source": [
    "fam_lt_1k_pac = homelessness[(homelessness['family_members'] < 1000) & (homelessness['region'] == 'Pacific')]\n",
    "print(fam_lt_1k_pac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0    region       state  individuals  family_members  state_pop\n",
      "2            2  Mountain     Arizona       7259.0          2606.0    7158024\n",
      "4            4   Pacific  California     109008.0         20964.0   39461588\n",
      "28          28  Mountain      Nevada       7058.0           486.0    3027341\n",
      "44          44  Mountain        Utah       1904.0           972.0    3153550\n"
     ]
    }
   ],
   "source": [
    "# The Mojave Desert states\n",
    "canu = [\"California\", \"Arizona\", \"Nevada\", \"Utah\"]\n",
    "\n",
    "mojave_homelessness = homelessness[homelessness['state'].isin(canu)]\n",
    "\n",
    "print(mojave_homelessness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0              region                 state  individuals  \\\n",
      "0            0  East South Central               Alabama       2570.0   \n",
      "1            1             Pacific                Alaska       1434.0   \n",
      "2            2            Mountain               Arizona       7259.0   \n",
      "3            3  West South Central              Arkansas       2280.0   \n",
      "4            4             Pacific            California     109008.0   \n",
      "5            5            Mountain              Colorado       7607.0   \n",
      "6            6         New England           Connecticut       2280.0   \n",
      "7            7      South Atlantic              Delaware        708.0   \n",
      "8            8      South Atlantic  District of Columbia       3770.0   \n",
      "9            9      South Atlantic               Florida      21443.0   \n",
      "10          10      South Atlantic               Georgia       6943.0   \n",
      "11          11             Pacific                Hawaii       4131.0   \n",
      "12          12            Mountain                 Idaho       1297.0   \n",
      "13          13  East North Central              Illinois       6752.0   \n",
      "14          14  East North Central               Indiana       3776.0   \n",
      "15          15  West North Central                  Iowa       1711.0   \n",
      "16          16  West North Central                Kansas       1443.0   \n",
      "17          17  East South Central              Kentucky       2735.0   \n",
      "18          18  West South Central             Louisiana       2540.0   \n",
      "19          19         New England                 Maine       1450.0   \n",
      "20          20      South Atlantic              Maryland       4914.0   \n",
      "21          21         New England         Massachusetts       6811.0   \n",
      "22          22  East North Central              Michigan       5209.0   \n",
      "23          23  West North Central             Minnesota       3993.0   \n",
      "24          24  East South Central           Mississippi       1024.0   \n",
      "25          25  West North Central              Missouri       3776.0   \n",
      "26          26            Mountain               Montana        983.0   \n",
      "27          27  West North Central              Nebraska       1745.0   \n",
      "28          28            Mountain                Nevada       7058.0   \n",
      "29          29         New England         New Hampshire        835.0   \n",
      "30          30        Mid-Atlantic            New Jersey       6048.0   \n",
      "31          31            Mountain            New Mexico       1949.0   \n",
      "32          32        Mid-Atlantic              New York      39827.0   \n",
      "33          33      South Atlantic        North Carolina       6451.0   \n",
      "34          34  West North Central          North Dakota        467.0   \n",
      "35          35  East North Central                  Ohio       6929.0   \n",
      "36          36  West South Central              Oklahoma       2823.0   \n",
      "37          37             Pacific                Oregon      11139.0   \n",
      "38          38        Mid-Atlantic          Pennsylvania       8163.0   \n",
      "39          39         New England          Rhode Island        747.0   \n",
      "40          40      South Atlantic        South Carolina       3082.0   \n",
      "41          41  West North Central          South Dakota        836.0   \n",
      "42          42  East South Central             Tennessee       6139.0   \n",
      "43          43  West South Central                 Texas      19199.0   \n",
      "44          44            Mountain                  Utah       1904.0   \n",
      "45          45         New England               Vermont        780.0   \n",
      "46          46      South Atlantic              Virginia       3928.0   \n",
      "47          47             Pacific            Washington      16424.0   \n",
      "48          48      South Atlantic         West Virginia       1021.0   \n",
      "49          49  East North Central             Wisconsin       2740.0   \n",
      "50          50            Mountain               Wyoming        434.0   \n",
      "\n",
      "    family_members  state_pop     total  p_individuals  p_homeless  \n",
      "0            864.0    4887681    3434.0       0.748398    0.000703  \n",
      "1            582.0     735139    2016.0       0.711310    0.002742  \n",
      "2           2606.0    7158024    9865.0       0.735834    0.001378  \n",
      "3            432.0    3009733    2712.0       0.840708    0.000901  \n",
      "4          20964.0   39461588  129972.0       0.838704    0.003294  \n",
      "5           3250.0    5691287   10857.0       0.700654    0.001908  \n",
      "6           1696.0    3571520    3976.0       0.573441    0.001113  \n",
      "7            374.0     965479    1082.0       0.654344    0.001121  \n",
      "8           3134.0     701547    6904.0       0.546060    0.009841  \n",
      "9           9587.0   21244317   31030.0       0.691041    0.001461  \n",
      "10          2556.0   10511131    9499.0       0.730919    0.000904  \n",
      "11          2399.0    1420593    6530.0       0.632619    0.004597  \n",
      "12           715.0    1750536    2012.0       0.644632    0.001149  \n",
      "13          3891.0   12723071   10643.0       0.634408    0.000837  \n",
      "14          1482.0    6695497    5258.0       0.718144    0.000785  \n",
      "15          1038.0    3148618    2749.0       0.622408    0.000873  \n",
      "16           773.0    2911359    2216.0       0.651173    0.000761  \n",
      "17           953.0    4461153    3688.0       0.741594    0.000827  \n",
      "18           519.0    4659690    3059.0       0.830337    0.000656  \n",
      "19          1066.0    1339057    2516.0       0.576312    0.001879  \n",
      "20          2230.0    6035802    7144.0       0.687850    0.001184  \n",
      "21         13257.0    6882635   20068.0       0.339396    0.002916  \n",
      "22          3142.0    9984072    8351.0       0.623758    0.000836  \n",
      "23          3250.0    5606249    7243.0       0.551291    0.001292  \n",
      "24           328.0    2981020    1352.0       0.757396    0.000454  \n",
      "25          2107.0    6121623    5883.0       0.641849    0.000961  \n",
      "26           422.0    1060665    1405.0       0.699644    0.001325  \n",
      "27           676.0    1925614    2421.0       0.720777    0.001257  \n",
      "28           486.0    3027341    7544.0       0.935578    0.002492  \n",
      "29           615.0    1353465    1450.0       0.575862    0.001071  \n",
      "30          3350.0    8886025    9398.0       0.643541    0.001058  \n",
      "31           602.0    2092741    2551.0       0.764014    0.001219  \n",
      "32         52070.0   19530351   91897.0       0.433387    0.004705  \n",
      "33          2817.0   10381615    9268.0       0.696051    0.000893  \n",
      "34            75.0     758080     542.0       0.861624    0.000715  \n",
      "35          3320.0   11676341   10249.0       0.676066    0.000878  \n",
      "36          1048.0    3940235    3871.0       0.729269    0.000982  \n",
      "37          3337.0    4181886   14476.0       0.769481    0.003462  \n",
      "38          5349.0   12800922   13512.0       0.604130    0.001056  \n",
      "39           354.0    1058287    1101.0       0.678474    0.001040  \n",
      "40           851.0    5084156    3933.0       0.783626    0.000774  \n",
      "41           323.0     878698    1159.0       0.721311    0.001319  \n",
      "42          1744.0    6771631    7883.0       0.778764    0.001164  \n",
      "43          6111.0   28628666   25310.0       0.758554    0.000884  \n",
      "44           972.0    3153550    2876.0       0.662031    0.000912  \n",
      "45           511.0     624358    1291.0       0.604183    0.002068  \n",
      "46          2047.0    8501286    5975.0       0.657406    0.000703  \n",
      "47          5880.0    7523869   22304.0       0.736370    0.002964  \n",
      "48           222.0    1804291    1243.0       0.821400    0.000689  \n",
      "49          2167.0    5807406    4907.0       0.558386    0.000845  \n",
      "50           205.0     577601     639.0       0.679186    0.001106  \n"
     ]
    }
   ],
   "source": [
    "homelessness['total'] = homelessness['individuals'] + homelessness['family_members']\n",
    "homelessness['p_homeless'] = homelessness['total'] / homelessness['state_pop']\n",
    "\n",
    "print(homelessness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   state  indiv_per_10k\n",
      "8   District of Columbia      53.738381\n",
      "11                Hawaii      29.079406\n",
      "4             California      27.623825\n",
      "37                Oregon      26.636307\n",
      "28                Nevada      23.314189\n",
      "47            Washington      21.829195\n",
      "32              New York      20.392363\n"
     ]
    }
   ],
   "source": [
    "# Create indiv_per_10k col as homeless individuals per 10k state pop\n",
    "homelessness[\"indiv_per_10k\"] = 10000 * homelessness['individuals'] / homelessness['state_pop'] \n",
    "\n",
    "# Subset rows for indiv_per_10k greater than 20\n",
    "high_homelessness = homelessness[homelessness['indiv_per_10k'] > 20]\n",
    "\n",
    "# Sort high_homelessness by descending indiv_per_10k\n",
    "high_homelessness_srt = high_homelessness.sort_values('indiv_per_10k', ascending=False)\n",
    "\n",
    "# From high_homelessness_srt, select the state and indiv_per_10k cols\n",
    "result = high_homelessness_srt[['state', 'indiv_per_10k']]\n",
    "\n",
    "# See the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('data/sales_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  store type  department        date  weekly_sales  is_holiday  \\\n",
      "0           0      1    A           1  2010-02-05      24924.50       False   \n",
      "1           1      1    A           1  2010-03-05      21827.90       False   \n",
      "2           2      1    A           1  2010-04-02      57258.43       False   \n",
      "3           3      1    A           1  2010-05-07      17413.94       False   \n",
      "4           4      1    A           1  2010-06-04      17558.09       False   \n",
      "\n",
      "   temperature_c  fuel_price_usd_per_l  unemployment  \n",
      "0       5.727778              0.679451         8.106  \n",
      "1       8.055556              0.693452         8.106  \n",
      "2      16.816667              0.718284         7.808  \n",
      "3      22.527778              0.748928         7.808  \n",
      "4      27.050000              0.714586         7.808  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10774 entries, 0 to 10773\n",
      "Data columns (total 10 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   Unnamed: 0            10774 non-null  int64  \n",
      " 1   store                 10774 non-null  int64  \n",
      " 2   type                  10774 non-null  object \n",
      " 3   department            10774 non-null  int64  \n",
      " 4   date                  10774 non-null  object \n",
      " 5   weekly_sales          10774 non-null  float64\n",
      " 6   is_holiday            10774 non-null  bool   \n",
      " 7   temperature_c         10774 non-null  float64\n",
      " 8   fuel_price_usd_per_l  10774 non-null  float64\n",
      " 9   unemployment          10774 non-null  float64\n",
      "dtypes: bool(1), float64(4), int64(3), object(2)\n",
      "memory usage: 768.2+ KB\n",
      "None\n",
      "23843.95014850566\n",
      "12049.064999999999\n"
     ]
    }
   ],
   "source": [
    "# Print the head of the sales DataFrame\n",
    "print(sales.head())\n",
    "\n",
    "# Print the info about the sales DataFrame\n",
    "print(sales.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23843.95014850566\n",
      "12049.064999999999\n"
     ]
    }
   ],
   "source": [
    "# Print the mean of weekly_sales\n",
    "print(sales['weekly_sales'].mean())\n",
    "\n",
    "# Print the median of weekly_sales\n",
    "print(sales['weekly_sales'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012-10-26\n",
      "2010-02-05\n"
     ]
    }
   ],
   "source": [
    "# Print the maximum of the date column\n",
    "print(sales['date'].max())\n",
    "\n",
    "# Print the minimum of the date column\n",
    "print(sales['date'].min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.583333333333336\n"
     ]
    }
   ],
   "source": [
    "# A custom IQR function\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "    \n",
    "# Print IQR of the temperature_c column\n",
    "print(sales['temperature_c'].agg(iqr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature_c           16.583333\n",
      "fuel_price_usd_per_l     0.073176\n",
      "unemployment             0.565000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# A custom IQR function\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Update to print IQR of temperature_c, fuel_price_usd_per_l, & unemployment\n",
    "print(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg(iqr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        temperature_c  fuel_price_usd_per_l  unemployment\n",
      "iqr         16.583333              0.073176         0.565\n",
      "median      16.966667              0.743381         8.099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/gmqchw852_z_pqpsn82qqrnm0000gn/T/ipykernel_28844/448815662.py:7: FutureWarning: The provided callable <function median at 0x106c97740> is currently using Series.median. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"median\" instead.\n",
      "  print(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr, np.median]))\n",
      "/var/folders/wv/gmqchw852_z_pqpsn82qqrnm0000gn/T/ipykernel_28844/448815662.py:7: FutureWarning: The provided callable <function median at 0x106c97740> is currently using Series.median. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"median\" instead.\n",
      "  print(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr, np.median]))\n",
      "/var/folders/wv/gmqchw852_z_pqpsn82qqrnm0000gn/T/ipykernel_28844/448815662.py:7: FutureWarning: The provided callable <function median at 0x106c97740> is currently using Series.median. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"median\" instead.\n",
      "  print(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr, np.median]))\n"
     ]
    }
   ],
   "source": [
    "# Import NumPy and create custom IQR function\n",
    "import numpy as np\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment\n",
    "print(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr, np.median]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date  weekly_sales  cum_weekly_sales  cum_max_sales\n",
      "0      2010-02-05      24924.50      2.492450e+04       24924.50\n",
      "6437   2010-02-05      38597.52      6.352202e+04       38597.52\n",
      "1249   2010-02-05       3840.21      6.736223e+04       38597.52\n",
      "6449   2010-02-05      17590.59      8.495282e+04       38597.52\n",
      "6461   2010-02-05       4929.87      8.988269e+04       38597.52\n",
      "...           ...           ...               ...            ...\n",
      "3592   2012-10-05        440.00      2.568932e+08      293966.05\n",
      "8108   2012-10-05        660.00      2.568938e+08      293966.05\n",
      "10773  2012-10-05        915.00      2.568947e+08      293966.05\n",
      "6257   2012-10-12          3.00      2.568947e+08      293966.05\n",
      "3384   2012-10-26        -21.63      2.568947e+08      293966.05\n",
      "\n",
      "[10774 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Sort sales_1_1 by date\n",
    "sales_1_1 = sales.sort_values('date')\n",
    "\n",
    "# Get the cumulative sum of weekly_sales, add as cum_weekly_sales col\n",
    "sales_1_1['cum_weekly_sales'] = sales_1_1['weekly_sales'].cumsum()\n",
    "\n",
    "# Get the cumulative max of weekly_sales, add as cum_max_sales col\n",
    "sales_1_1['cum_max_sales'] = sales_1_1['weekly_sales'].cummax()\n",
    "\n",
    "# See the columns you calculated\n",
    "print(sales_1_1[[\"date\", \"weekly_sales\", \"cum_weekly_sales\", \"cum_max_sales\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0  store type  department        date  weekly_sales  \\\n",
      "0              0      1    A           1  2010-02-05      24924.50   \n",
      "901          901      2    A           1  2010-02-05      35034.06   \n",
      "1798        1798      4    A           1  2010-02-05      38724.42   \n",
      "2699        2699      6    A           1  2010-02-05      25619.00   \n",
      "3593        3593     10    B           1  2010-02-05      40212.84   \n",
      "\n",
      "      is_holiday  temperature_c  fuel_price_usd_per_l  unemployment  \n",
      "0          False       5.727778              0.679451         8.106  \n",
      "901        False       4.550000              0.679451         8.324  \n",
      "1798       False       6.533333              0.686319         8.623  \n",
      "2699       False       4.683333              0.679451         7.259  \n",
      "3593       False      12.411111              0.782478         9.765  \n",
      "    Unnamed: 0  store type  department        date  weekly_sales  is_holiday  \\\n",
      "0            0      1    A           1  2010-02-05      24924.50       False   \n",
      "12          12      1    A           2  2010-02-05      50605.27       False   \n",
      "24          24      1    A           3  2010-02-05      13740.12       False   \n",
      "36          36      1    A           4  2010-02-05      39954.04       False   \n",
      "48          48      1    A           5  2010-02-05      32229.38       False   \n",
      "\n",
      "    temperature_c  fuel_price_usd_per_l  unemployment  \n",
      "0        5.727778              0.679451         8.106  \n",
      "12       5.727778              0.679451         8.106  \n",
      "24       5.727778              0.679451         8.106  \n",
      "36       5.727778              0.679451         8.106  \n",
      "48       5.727778              0.679451         8.106  \n",
      "      Unnamed: 0  store type  department        date  weekly_sales  \\\n",
      "498          498      1    A          45  2010-09-10         11.47   \n",
      "691          691      1    A          77  2011-11-25       1431.00   \n",
      "2315        2315      4    A          47  2010-02-12        498.00   \n",
      "6735        6735     19    A          39  2012-09-07         13.41   \n",
      "6810        6810     19    A          47  2010-12-31       -449.00   \n",
      "6815        6815     19    A          47  2012-02-10         15.00   \n",
      "6820        6820     19    A          48  2011-09-09        197.00   \n",
      "\n",
      "      is_holiday  temperature_c  fuel_price_usd_per_l  unemployment  \n",
      "498         True      25.938889              0.677602         7.787  \n",
      "691         True      15.633333              0.854861         7.866  \n",
      "2315        True      -1.755556              0.679715         8.623  \n",
      "6735        True      22.333333              1.076766         8.193  \n",
      "6810        True      -1.861111              0.881278         8.067  \n",
      "6815        True       0.338889              1.010723         7.943  \n",
      "6820        True      20.155556              1.038197         7.806  \n"
     ]
    }
   ],
   "source": [
    "# Drop duplicate store/type combinations\n",
    "store_types = sales.drop_duplicates(subset=['store', 'type'])\n",
    "print(store_types.head())\n",
    "\n",
    "# Drop duplicate store/department combinations\n",
    "store_depts = sales.drop_duplicates(subset=['store','department'])\n",
    "print(store_depts.head())\n",
    "\n",
    "# Subset the rows where is_holiday is True and drop duplicate dates\n",
    "holiday_dates = sales[sales['is_holiday'] == True].drop_duplicates(subset=['date'])\n",
    "\n",
    "# Print date col of holiday_dates\n",
    "print(holiday_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "A    11\n",
      "B     1\n",
      "Name: count, dtype: int64\n",
      "type\n",
      "A    0.916667\n",
      "B    0.083333\n",
      "Name: proportion, dtype: float64\n",
      "department\n",
      "1     12\n",
      "55    12\n",
      "72    12\n",
      "71    12\n",
      "67    12\n",
      "      ..\n",
      "37    10\n",
      "48     8\n",
      "50     6\n",
      "39     4\n",
      "43     2\n",
      "Name: count, Length: 80, dtype: int64\n",
      "department\n",
      "1     0.012917\n",
      "55    0.012917\n",
      "72    0.012917\n",
      "71    0.012917\n",
      "67    0.012917\n",
      "        ...   \n",
      "37    0.010764\n",
      "48    0.008611\n",
      "50    0.006459\n",
      "39    0.004306\n",
      "43    0.002153\n",
      "Name: proportion, Length: 80, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Count the number of stores of each type\n",
    "store_counts = store_types['type'].value_counts()\n",
    "print(store_counts)\n",
    "\n",
    "# Get the proportion of stores of each type\n",
    "store_props = store_types['type'].value_counts(normalize=True)\n",
    "print(store_props)\n",
    "\n",
    "# Count the number of stores for each department and sort\n",
    "dept_counts_sorted = store_depts['department'].value_counts(sort=True)\n",
    "print(dept_counts_sorted)\n",
    "\n",
    "# Get the proportion of stores in each department and sort\n",
    "dept_props_sorted = store_depts['department'].value_counts(sort=True, normalize=True)\n",
    "print(dept_props_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9097747 0.0902253 0.       ]\n"
     ]
    }
   ],
   "source": [
    "# Calc total weekly sales\n",
    "sales_all = sales[\"weekly_sales\"].sum()\n",
    "\n",
    "# Subset for type A stores, calc total weekly sales\n",
    "sales_A = sales[sales[\"type\"] == \"A\"][\"weekly_sales\"].sum()\n",
    "\n",
    "# Subset for type B stores, calc total weekly sales\n",
    "sales_B = sales[sales[\"type\"] == \"B\"][\"weekly_sales\"].sum()\n",
    "\n",
    "# Subset for type C stores, calc total weekly sales\n",
    "sales_C = sales[sales[\"type\"] == \"C\"][\"weekly_sales\"].sum()\n",
    "\n",
    "# Get proportion for each type\n",
    "sales_propn_by_type = [sales_A, sales_B, sales_C] / sales_all\n",
    "print(sales_propn_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "A    0.909775\n",
      "B    0.090225\n",
      "Name: weekly_sales, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Group by type; calc total weekly sales\n",
    "sales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n",
    "\n",
    "# Get proportion for each type\n",
    "sales_propn_by_type = sales_by_type / sum(sales_by_type)\n",
    "print(sales_propn_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type  is_holiday\n",
      "A     False         2.336927e+08\n",
      "      True          2.360181e+04\n",
      "B     False         2.317678e+07\n",
      "      True          1.621410e+03\n",
      "Name: weekly_sales, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# From previous step\n",
    "sales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n",
    "\n",
    "# Group by type and is_holiday; calc total weekly sales\n",
    "sales_by_type_is_holiday = sales.groupby(['type', 'is_holiday'])['weekly_sales'].sum()\n",
    "print(sales_by_type_is_holiday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         min        max          mean    median\n",
      "type                                           \n",
      "A    -1098.0  293966.05  23674.667242  11943.92\n",
      "B     -798.0  232558.51  25696.678370  13336.08\n",
      "     unemployment                         fuel_price_usd_per_l            \\\n",
      "              min    max      mean median                  min       max   \n",
      "type                                                                       \n",
      "A           3.879  8.992  7.972611  8.067             0.664129  1.107410   \n",
      "B           7.170  9.765  9.279323  9.199             0.760023  1.107674   \n",
      "\n",
      "                          \n",
      "          mean    median  \n",
      "type                      \n",
      "A     0.744619  0.735455  \n",
      "B     0.805858  0.803348  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/gmqchw852_z_pqpsn82qqrnm0000gn/T/ipykernel_28844/2119523908.py:5: FutureWarning: The provided callable <function min at 0x106b3dbc0> is currently using SeriesGroupBy.min. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"min\" instead.\n",
      "  sales_stats = sales.groupby('type')['weekly_sales'].agg([np.min, np.max, np.mean, np.median])\n",
      "/var/folders/wv/gmqchw852_z_pqpsn82qqrnm0000gn/T/ipykernel_28844/2119523908.py:5: FutureWarning: The provided callable <function max at 0x106b3da80> is currently using SeriesGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  sales_stats = sales.groupby('type')['weekly_sales'].agg([np.min, np.max, np.mean, np.median])\n",
      "/var/folders/wv/gmqchw852_z_pqpsn82qqrnm0000gn/T/ipykernel_28844/2119523908.py:5: FutureWarning: The provided callable <function mean at 0x106b3e480> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  sales_stats = sales.groupby('type')['weekly_sales'].agg([np.min, np.max, np.mean, np.median])\n",
      "/var/folders/wv/gmqchw852_z_pqpsn82qqrnm0000gn/T/ipykernel_28844/2119523908.py:5: FutureWarning: The provided callable <function median at 0x106c97740> is currently using SeriesGroupBy.median. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"median\" instead.\n",
      "  sales_stats = sales.groupby('type')['weekly_sales'].agg([np.min, np.max, np.mean, np.median])\n",
      "/var/folders/wv/gmqchw852_z_pqpsn82qqrnm0000gn/T/ipykernel_28844/2119523908.py:11: FutureWarning: The provided callable <function min at 0x106b3dbc0> is currently using SeriesGroupBy.min. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"min\" instead.\n",
      "  unemp_fuel_stats = sales.groupby('type')[['unemployment', 'fuel_price_usd_per_l']].agg([np.min, np.max, np.mean, np.median])\n",
      "/var/folders/wv/gmqchw852_z_pqpsn82qqrnm0000gn/T/ipykernel_28844/2119523908.py:11: FutureWarning: The provided callable <function max at 0x106b3da80> is currently using SeriesGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  unemp_fuel_stats = sales.groupby('type')[['unemployment', 'fuel_price_usd_per_l']].agg([np.min, np.max, np.mean, np.median])\n",
      "/var/folders/wv/gmqchw852_z_pqpsn82qqrnm0000gn/T/ipykernel_28844/2119523908.py:11: FutureWarning: The provided callable <function mean at 0x106b3e480> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  unemp_fuel_stats = sales.groupby('type')[['unemployment', 'fuel_price_usd_per_l']].agg([np.min, np.max, np.mean, np.median])\n",
      "/var/folders/wv/gmqchw852_z_pqpsn82qqrnm0000gn/T/ipykernel_28844/2119523908.py:11: FutureWarning: The provided callable <function median at 0x106c97740> is currently using SeriesGroupBy.median. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"median\" instead.\n",
      "  unemp_fuel_stats = sales.groupby('type')[['unemployment', 'fuel_price_usd_per_l']].agg([np.min, np.max, np.mean, np.median])\n",
      "/var/folders/wv/gmqchw852_z_pqpsn82qqrnm0000gn/T/ipykernel_28844/2119523908.py:11: FutureWarning: The provided callable <function min at 0x106b3dbc0> is currently using SeriesGroupBy.min. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"min\" instead.\n",
      "  unemp_fuel_stats = sales.groupby('type')[['unemployment', 'fuel_price_usd_per_l']].agg([np.min, np.max, np.mean, np.median])\n"
     ]
    }
   ],
   "source": [
    "# Import numpy with the alias np\n",
    "import numpy as np\n",
    "\n",
    "# For each store type, aggregate weekly_sales: get min, max, mean, and median\n",
    "sales_stats = sales.groupby('type')['weekly_sales'].agg([np.min, np.max, np.mean, np.median])\n",
    "\n",
    "# Print sales_stats\n",
    "print(sales_stats)\n",
    "\n",
    "# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\n",
    "unemp_fuel_stats = sales.groupby('type')[['unemployment', 'fuel_price_usd_per_l']].agg([np.min, np.max, np.mean, np.median])\n",
    "\n",
    "# Print unemp_fuel_stats\n",
    "print(unemp_fuel_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      weekly_sales\n",
      "type              \n",
      "A     23674.667242\n",
      "B     25696.678370\n"
     ]
    }
   ],
   "source": [
    "# Pivot for mean weekly_sales for each store type\n",
    "mean_sales_by_type = sales.pivot_table(values='weekly_sales', index='type')\n",
    "\n",
    "# Print mean_sales_by_type\n",
    "print(mean_sales_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              mean       median\n",
      "      weekly_sales weekly_sales\n",
      "type                           \n",
      "A     23674.667242     11943.92\n",
      "B     25696.678370     13336.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/gmqchw852_z_pqpsn82qqrnm0000gn/T/ipykernel_28844/2784005085.py:5: FutureWarning: The provided callable <function mean at 0x106b3e480> is currently using DataFrameGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  mean_med_sales_by_type = sales.pivot_table(values='weekly_sales', index='type', aggfunc=[np.mean, np.median])\n",
      "/var/folders/wv/gmqchw852_z_pqpsn82qqrnm0000gn/T/ipykernel_28844/2784005085.py:5: FutureWarning: The provided callable <function median at 0x106c97740> is currently using DataFrameGroupBy.median. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"median\" instead.\n",
      "  mean_med_sales_by_type = sales.pivot_table(values='weekly_sales', index='type', aggfunc=[np.mean, np.median])\n"
     ]
    }
   ],
   "source": [
    "# Import NumPy as np\n",
    "import numpy as np\n",
    "\n",
    "# Pivot for mean and median weekly_sales for each store type\n",
    "mean_med_sales_by_type = sales.pivot_table(values='weekly_sales', index='type', aggfunc=[np.mean, np.median])\n",
    "\n",
    "# Print mean_med_sales_by_type\n",
    "print(mean_med_sales_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_holiday         False      True \n",
      "type                               \n",
      "A           23768.583523  590.04525\n",
      "B           25751.980533  810.70500\n"
     ]
    }
   ],
   "source": [
    "# Pivot for mean weekly_sales by store type and holiday \n",
    "mean_sales_by_type_holiday = sales.pivot_table(values='weekly_sales', index='type', columns='is_holiday')\n",
    "\n",
    "# Print mean_sales_by_type_holiday\n",
    "print(mean_sales_by_type_holiday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type                    A              B\n",
      "department                              \n",
      "1            30961.725379   44050.626667\n",
      "2            67600.158788  112958.526667\n",
      "3            17160.002955   30580.655000\n",
      "4            44285.399091   51219.654167\n",
      "5            34821.011364   63236.875000\n",
      "...                   ...            ...\n",
      "95          123933.787121   77082.102500\n",
      "96           21367.042857    9528.538333\n",
      "97           28471.266970    5828.873333\n",
      "98           12875.423182     217.428333\n",
      "99             379.123659       0.000000\n",
      "\n",
      "[80 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print mean weekly_sales by department and type; fill missing values with 0\n",
    "print(sales.pivot_table(values='weekly_sales', index='department', columns='type', fill_value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type                   A              B           All\n",
      "department                                           \n",
      "1           30961.725379   44050.626667  32052.467153\n",
      "2           67600.158788  112958.526667  71380.022778\n",
      "3           17160.002955   30580.655000  18278.390625\n",
      "4           44285.399091   51219.654167  44863.253681\n",
      "5           34821.011364   63236.875000  37189.000000\n",
      "...                  ...            ...           ...\n",
      "96          21367.042857    9528.538333  20337.607681\n",
      "97          28471.266970    5828.873333  26584.400833\n",
      "98          12875.423182     217.428333  11820.590278\n",
      "99            379.123659       0.000000    379.123659\n",
      "All         23674.667242   25696.678370  23843.950149\n",
      "\n",
      "[81 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols\n",
    "print(sales.pivot_table(values=\"weekly_sales\", index=\"department\", columns=\"type\", fill_value=0, margins=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = pd.read_csv('data/temperatures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0        date     city        country  avg_temp_c\n",
      "0           0  2000-01-01  Abidjan  Cte D'Ivoire      27.293\n",
      "1           1  2000-02-01  Abidjan  Cte D'Ivoire      27.685\n",
      "2           2  2000-03-01  Abidjan  Cte D'Ivoire      29.061\n",
      "3           3  2000-04-01  Abidjan  Cte D'Ivoire      28.162\n",
      "4           4  2000-05-01  Abidjan  Cte D'Ivoire      27.547\n",
      "         Unnamed: 0        date        country  avg_temp_c\n",
      "city                                                      \n",
      "Abidjan           0  2000-01-01  Cte D'Ivoire      27.293\n",
      "Abidjan           1  2000-02-01  Cte D'Ivoire      27.685\n",
      "Abidjan           2  2000-03-01  Cte D'Ivoire      29.061\n",
      "Abidjan           3  2000-04-01  Cte D'Ivoire      28.162\n",
      "Abidjan           4  2000-05-01  Cte D'Ivoire      27.547\n",
      "          city  Unnamed: 0        date        country  avg_temp_c\n",
      "0      Abidjan           0  2000-01-01  Cte D'Ivoire      27.293\n",
      "1      Abidjan           1  2000-02-01  Cte D'Ivoire      27.685\n",
      "2      Abidjan           2  2000-03-01  Cte D'Ivoire      29.061\n",
      "3      Abidjan           3  2000-04-01  Cte D'Ivoire      28.162\n",
      "4      Abidjan           4  2000-05-01  Cte D'Ivoire      27.547\n",
      "...        ...         ...         ...            ...         ...\n",
      "16495     Xian       16495  2013-05-01          China      18.979\n",
      "16496     Xian       16496  2013-06-01          China      23.522\n",
      "16497     Xian       16497  2013-07-01          China      25.251\n",
      "16498     Xian       16498  2013-08-01          China      24.528\n",
      "16499     Xian       16499  2013-09-01          China         NaN\n",
      "\n",
      "[16500 rows x 5 columns]\n",
      "       Unnamed: 0        date        country  avg_temp_c\n",
      "0               0  2000-01-01  Cte D'Ivoire      27.293\n",
      "1               1  2000-02-01  Cte D'Ivoire      27.685\n",
      "2               2  2000-03-01  Cte D'Ivoire      29.061\n",
      "3               3  2000-04-01  Cte D'Ivoire      28.162\n",
      "4               4  2000-05-01  Cte D'Ivoire      27.547\n",
      "...           ...         ...            ...         ...\n",
      "16495       16495  2013-05-01          China      18.979\n",
      "16496       16496  2013-06-01          China      23.522\n",
      "16497       16497  2013-07-01          China      25.251\n",
      "16498       16498  2013-08-01          China      24.528\n",
      "16499       16499  2013-09-01          China         NaN\n",
      "\n",
      "[16500 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Look at temperatures\n",
    "print(temperatures.head())\n",
    "\n",
    "# Set the index of temperatures to city\n",
    "temperatures_ind = temperatures.set_index('city')\n",
    "\n",
    "# Look at temperatures_ind\n",
    "print(temperatures_ind.head())\n",
    "\n",
    "# Reset the temperatures_ind index, keeping its contents\n",
    "print(temperatures_ind.reset_index())\n",
    "\n",
    "# Reset the temperatures_ind index, dropping its contents\n",
    "print(temperatures_ind.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0        date              city country  avg_temp_c\n",
      "10725       10725  2000-01-01            Moscow  Russia      -7.313\n",
      "10726       10726  2000-02-01            Moscow  Russia      -3.551\n",
      "10727       10727  2000-03-01            Moscow  Russia      -1.661\n",
      "10728       10728  2000-04-01            Moscow  Russia      10.096\n",
      "10729       10729  2000-05-01            Moscow  Russia      10.357\n",
      "...           ...         ...               ...     ...         ...\n",
      "13360       13360  2013-05-01  Saint Petersburg  Russia      12.355\n",
      "13361       13361  2013-06-01  Saint Petersburg  Russia      17.185\n",
      "13362       13362  2013-07-01  Saint Petersburg  Russia      17.234\n",
      "13363       13363  2013-08-01  Saint Petersburg  Russia      17.153\n",
      "13364       13364  2013-09-01  Saint Petersburg  Russia         NaN\n",
      "\n",
      "[330 rows x 5 columns]\n",
      "                  Unnamed: 0        date country  avg_temp_c\n",
      "city                                                        \n",
      "Moscow                 10725  2000-01-01  Russia      -7.313\n",
      "Moscow                 10726  2000-02-01  Russia      -3.551\n",
      "Moscow                 10727  2000-03-01  Russia      -1.661\n",
      "Moscow                 10728  2000-04-01  Russia      10.096\n",
      "Moscow                 10729  2000-05-01  Russia      10.357\n",
      "...                      ...         ...     ...         ...\n",
      "Saint Petersburg       13360  2013-05-01  Russia      12.355\n",
      "Saint Petersburg       13361  2013-06-01  Russia      17.185\n",
      "Saint Petersburg       13362  2013-07-01  Russia      17.234\n",
      "Saint Petersburg       13363  2013-08-01  Russia      17.153\n",
      "Saint Petersburg       13364  2013-09-01  Russia         NaN\n",
      "\n",
      "[330 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Make a list of cities to subset on\n",
    "cities = [\"Moscow\", \"Saint Petersburg\"]\n",
    "\n",
    "# Subset temperatures using square brackets\n",
    "print(temperatures[temperatures['city'].isin(cities)])\n",
    "\n",
    "# Subset temperatures_ind using .loc[]\n",
    "print(temperatures_ind.loc[cities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Unnamed: 0        date  avg_temp_c\n",
      "country  city                                              \n",
      "Brazil   Rio De Janeiro       12540  2000-01-01      25.974\n",
      "         Rio De Janeiro       12541  2000-02-01      26.699\n",
      "         Rio De Janeiro       12542  2000-03-01      26.270\n",
      "         Rio De Janeiro       12543  2000-04-01      25.750\n",
      "         Rio De Janeiro       12544  2000-05-01      24.356\n",
      "...                             ...         ...         ...\n",
      "Pakistan Lahore                8575  2013-05-01      33.457\n",
      "         Lahore                8576  2013-06-01      34.456\n",
      "         Lahore                8577  2013-07-01      33.279\n",
      "         Lahore                8578  2013-08-01      31.511\n",
      "         Lahore                8579  2013-09-01         NaN\n",
      "\n",
      "[330 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Index temperatures by country & city\n",
    "temperatures_ind = temperatures.set_index(['country', 'city'])\n",
    "\n",
    "# List of tuples: Brazil, Rio De Janeiro & Pakistan, Lahore\n",
    "rows_to_keep = [(\"Brazil\", \"Rio De Janeiro\"), (\"Pakistan\", \"Lahore\")]\n",
    "\n",
    "# Subset for rows to keep\n",
    "print(temperatures_ind.loc[rows_to_keep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Unnamed: 0        date  avg_temp_c\n",
      "country     city                                      \n",
      "Afghanistan Kabul         7260  2000-01-01       3.326\n",
      "            Kabul         7261  2000-02-01       3.454\n",
      "            Kabul         7262  2000-03-01       9.612\n",
      "            Kabul         7263  2000-04-01      17.925\n",
      "            Kabul         7264  2000-05-01      24.658\n",
      "...                        ...         ...         ...\n",
      "Zimbabwe    Harare        5605  2013-05-01      18.298\n",
      "            Harare        5606  2013-06-01      17.020\n",
      "            Harare        5607  2013-07-01      16.299\n",
      "            Harare        5608  2013-08-01      19.232\n",
      "            Harare        5609  2013-09-01         NaN\n",
      "\n",
      "[16500 rows x 3 columns]\n",
      "                       Unnamed: 0        date  avg_temp_c\n",
      "country       city                                       \n",
      "Cte D'Ivoire Abidjan           0  2000-01-01      27.293\n",
      "              Abidjan           1  2000-02-01      27.685\n",
      "              Abidjan           2  2000-03-01      29.061\n",
      "              Abidjan           3  2000-04-01      28.162\n",
      "              Abidjan           4  2000-05-01      27.547\n",
      "...                           ...         ...         ...\n",
      "China         Xian          16495  2013-05-01      18.979\n",
      "              Xian          16496  2013-06-01      23.522\n",
      "              Xian          16497  2013-07-01      25.251\n",
      "              Xian          16498  2013-08-01      24.528\n",
      "              Xian          16499  2013-09-01         NaN\n",
      "\n",
      "[16500 rows x 3 columns]\n",
      "                    Unnamed: 0        date  avg_temp_c\n",
      "country     city                                      \n",
      "Afghanistan Kabul         7260  2000-01-01       3.326\n",
      "            Kabul         7261  2000-02-01       3.454\n",
      "            Kabul         7262  2000-03-01       9.612\n",
      "            Kabul         7263  2000-04-01      17.925\n",
      "            Kabul         7264  2000-05-01      24.658\n",
      "...                        ...         ...         ...\n",
      "Zimbabwe    Harare        5605  2013-05-01      18.298\n",
      "            Harare        5606  2013-06-01      17.020\n",
      "            Harare        5607  2013-07-01      16.299\n",
      "            Harare        5608  2013-08-01      19.232\n",
      "            Harare        5609  2013-09-01         NaN\n",
      "\n",
      "[16500 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Sort temperatures_ind by index values\n",
    "print(temperatures_ind.sort_index())\n",
    "\n",
    "# Sort temperatures_ind by index values at the city level\n",
    "print(temperatures_ind.sort_index(level='city'))\n",
    "\n",
    "# Sort temperatures_ind by country then descending city\n",
    "print(temperatures_ind.sort_index(level=['country','city'], ascending=[True, False]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Unnamed: 0        date  avg_temp_c\n",
      "country  city                                                \n",
      "Pakistan Faisalabad              4785  2000-01-01      12.792\n",
      "         Faisalabad              4786  2000-02-01      14.339\n",
      "         Faisalabad              4787  2000-03-01      20.309\n",
      "         Faisalabad              4788  2000-04-01      29.072\n",
      "         Faisalabad              4789  2000-05-01      34.845\n",
      "...                               ...         ...         ...\n",
      "Russia   Saint Petersburg       13360  2013-05-01      12.355\n",
      "         Saint Petersburg       13361  2013-06-01      17.185\n",
      "         Saint Petersburg       13362  2013-07-01      17.234\n",
      "         Saint Petersburg       13363  2013-08-01      17.153\n",
      "         Saint Petersburg       13364  2013-09-01         NaN\n",
      "\n",
      "[1155 rows x 3 columns]\n",
      "                    Unnamed: 0        date  avg_temp_c\n",
      "country city                                          \n",
      "Mexico  Mexico           10230  2000-01-01      12.694\n",
      "        Mexico           10231  2000-02-01      14.677\n",
      "        Mexico           10232  2000-03-01      17.376\n",
      "        Mexico           10233  2000-04-01      18.294\n",
      "        Mexico           10234  2000-05-01      18.562\n",
      "...                        ...         ...         ...\n",
      "Morocco Casablanca        3130  2013-05-01      19.217\n",
      "        Casablanca        3131  2013-06-01      23.649\n",
      "        Casablanca        3132  2013-07-01      27.488\n",
      "        Casablanca        3133  2013-08-01      27.952\n",
      "        Casablanca        3134  2013-09-01         NaN\n",
      "\n",
      "[330 rows x 3 columns]\n",
      "                 Unnamed: 0        date  avg_temp_c\n",
      "country  city                                      \n",
      "Pakistan Lahore        8415  2000-01-01      12.792\n",
      "         Lahore        8416  2000-02-01      14.339\n",
      "         Lahore        8417  2000-03-01      20.309\n",
      "         Lahore        8418  2000-04-01      29.072\n",
      "         Lahore        8419  2000-05-01      34.845\n",
      "...                     ...         ...         ...\n",
      "Russia   Moscow       10885  2013-05-01      16.152\n",
      "         Moscow       10886  2013-06-01      18.718\n",
      "         Moscow       10887  2013-07-01      18.136\n",
      "         Moscow       10888  2013-08-01      17.485\n",
      "         Moscow       10889  2013-09-01         NaN\n",
      "\n",
      "[660 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Sort the index of temperatures_ind\n",
    "temperatures_srt = temperatures_ind.sort_index()\n",
    "\n",
    "# Subset rows from Pakistan to Russia\n",
    "print(temperatures_srt.loc['Pakistan':'Russia'])\n",
    "\n",
    "# Try to subset rows from Lahore to Moscow\n",
    "print(temperatures_srt.loc['Lahore':'Moscow'])\n",
    "\n",
    "# # Subset rows from Pakistan, Lahore to Russia, Moscow\n",
    "print(temperatures_srt.loc[('Pakistan', 'Lahore'):('Russia', 'Moscow')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Unnamed: 0        date  avg_temp_c\n",
      "country city                                         \n",
      "India   Hyderabad        5940  2000-01-01      23.779\n",
      "        Hyderabad        5941  2000-02-01      25.826\n",
      "        Hyderabad        5942  2000-03-01      28.821\n",
      "        Hyderabad        5943  2000-04-01      32.698\n",
      "        Hyderabad        5944  2000-05-01      32.438\n",
      "...                       ...         ...         ...\n",
      "Iraq    Baghdad          1150  2013-05-01      28.673\n",
      "        Baghdad          1151  2013-06-01      33.803\n",
      "        Baghdad          1152  2013-07-01      36.392\n",
      "        Baghdad          1153  2013-08-01      35.463\n",
      "        Baghdad          1154  2013-09-01         NaN\n",
      "\n",
      "[2145 rows x 3 columns]\n",
      "                          date  avg_temp_c\n",
      "country     city                          \n",
      "Afghanistan Kabul   2000-01-01       3.326\n",
      "            Kabul   2000-02-01       3.454\n",
      "            Kabul   2000-03-01       9.612\n",
      "            Kabul   2000-04-01      17.925\n",
      "            Kabul   2000-05-01      24.658\n",
      "...                        ...         ...\n",
      "Zimbabwe    Harare  2013-05-01      18.298\n",
      "            Harare  2013-06-01      17.020\n",
      "            Harare  2013-07-01      16.299\n",
      "            Harare  2013-08-01      19.232\n",
      "            Harare  2013-09-01         NaN\n",
      "\n",
      "[16500 rows x 2 columns]\n",
      "                         date  avg_temp_c\n",
      "country city                             \n",
      "India   Hyderabad  2000-01-01      23.779\n",
      "        Hyderabad  2000-02-01      25.826\n",
      "        Hyderabad  2000-03-01      28.821\n",
      "        Hyderabad  2000-04-01      32.698\n",
      "        Hyderabad  2000-05-01      32.438\n",
      "...                       ...         ...\n",
      "Iraq    Baghdad    2013-05-01      28.673\n",
      "        Baghdad    2013-06-01      33.803\n",
      "        Baghdad    2013-07-01      36.392\n",
      "        Baghdad    2013-08-01      35.463\n",
      "        Baghdad    2013-09-01         NaN\n",
      "\n",
      "[2145 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Subset rows from India, Hyderabad to Iraq, Baghdad\n",
    "print(temperatures_srt.loc[('India', 'Hyderabad'):('Iraq', 'Baghdad')])\n",
    "\n",
    "# Subset columns from date to avg_temp_c\n",
    "print(temperatures_srt.loc[:, 'date':'avg_temp_c'])\n",
    "\n",
    "# Subset in both directions at once\n",
    "print(temperatures_srt.loc[('India', 'Hyderabad'):('Iraq', 'Baghdad'), 'date':'avg_temp_c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0        date     city        country  avg_temp_c\n",
      "120           120  2010-01-01  Abidjan  Cte D'Ivoire      28.270\n",
      "121           121  2010-02-01  Abidjan  Cte D'Ivoire      29.262\n",
      "122           122  2010-03-01  Abidjan  Cte D'Ivoire      29.596\n",
      "123           123  2010-04-01  Abidjan  Cte D'Ivoire      29.068\n",
      "124           124  2010-05-01  Abidjan  Cte D'Ivoire      28.258\n",
      "...           ...         ...      ...            ...         ...\n",
      "16474       16474  2011-08-01     Xian          China      23.069\n",
      "16475       16475  2011-09-01     Xian          China      16.775\n",
      "16476       16476  2011-10-01     Xian          China      12.587\n",
      "16477       16477  2011-11-01     Xian          China       7.543\n",
      "16478       16478  2011-12-01     Xian          China      -0.490\n",
      "\n",
      "[2400 rows x 5 columns]\n",
      "            Unnamed: 0        city    country  avg_temp_c\n",
      "date                                                     \n",
      "2010-01-01        4905  Faisalabad   Pakistan      11.810\n",
      "2010-01-01       10185   Melbourne  Australia      20.016\n",
      "2010-01-01        3750   Chongqing      China       7.921\n",
      "2010-01-01       13155   So Paulo     Brazil      23.738\n",
      "2010-01-01        5400   Guangzhou      China      14.136\n",
      "...                ...         ...        ...         ...\n",
      "2010-12-01        6896     Jakarta  Indonesia      26.602\n",
      "2010-12-01        5246       Gizeh      Egypt      16.530\n",
      "2010-12-01       11186      Nagpur      India      19.120\n",
      "2010-12-01       14981      Sydney  Australia      19.559\n",
      "2010-12-01       13496    Salvador     Brazil      26.265\n",
      "\n",
      "[1200 rows x 4 columns]\n",
      "            Unnamed: 0           city        country  avg_temp_c\n",
      "date                                                            \n",
      "2010-08-01        2602       Calcutta          India      30.226\n",
      "2010-08-01       12337           Pune          India      24.941\n",
      "2010-08-01        6562          Izmir         Turkey      28.352\n",
      "2010-08-01       15637        Tianjin          China      25.543\n",
      "2010-08-01        9862         Manila    Philippines      27.101\n",
      "...                ...            ...            ...         ...\n",
      "2011-01-01        4257  Dar Es Salaam       Tanzania      28.541\n",
      "2011-01-01       11352        Nairobi          Kenya      17.768\n",
      "2011-01-01         297    Addis Abeba       Ethiopia      17.708\n",
      "2011-01-01       11517        Nanjing          China       0.144\n",
      "2011-01-01       11847       New York  United States      -4.463\n",
      "\n",
      "[600 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Use Boolean conditions to subset temperatures for rows in 2010 and 2011\n",
    "temperatures_bool = temperatures[(temperatures['date'] >= '2010-01-01') & (temperatures['date'] <= '2011-12-31')]\n",
    "print(temperatures_bool)\n",
    "\n",
    "# Set date as the index and sort the index\n",
    "temperatures_ind = temperatures.set_index('date').sort_index()\n",
    "\n",
    "# Use .loc[] to subset temperatures_ind for rows in 2010 and 2011\n",
    "print(temperatures_ind.loc['2010':'2011'])\n",
    "\n",
    "# Use .loc[] to subset temperatures_ind for rows from Aug 2010 to Feb 2011\n",
    "print(temperatures_ind.loc['2010-08':'2011-02'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001-11-01\n",
      "   Unnamed: 0        date     city        country  avg_temp_c\n",
      "0           0  2000-01-01  Abidjan  Cte D'Ivoire      27.293\n",
      "1           1  2000-02-01  Abidjan  Cte D'Ivoire      27.685\n",
      "2           2  2000-03-01  Abidjan  Cte D'Ivoire      29.061\n",
      "3           3  2000-04-01  Abidjan  Cte D'Ivoire      28.162\n",
      "4           4  2000-05-01  Abidjan  Cte D'Ivoire      27.547\n",
      "          city        country\n",
      "0      Abidjan  Cte D'Ivoire\n",
      "1      Abidjan  Cte D'Ivoire\n",
      "2      Abidjan  Cte D'Ivoire\n",
      "3      Abidjan  Cte D'Ivoire\n",
      "4      Abidjan  Cte D'Ivoire\n",
      "...        ...            ...\n",
      "16495     Xian          China\n",
      "16496     Xian          China\n",
      "16497     Xian          China\n",
      "16498     Xian          China\n",
      "16499     Xian          China\n",
      "\n",
      "[16500 rows x 2 columns]\n",
      "      city        country\n",
      "0  Abidjan  Cte D'Ivoire\n",
      "1  Abidjan  Cte D'Ivoire\n",
      "2  Abidjan  Cte D'Ivoire\n",
      "3  Abidjan  Cte D'Ivoire\n",
      "4  Abidjan  Cte D'Ivoire\n"
     ]
    }
   ],
   "source": [
    "# Get 23rd row, 2nd column (index 22, 1)\n",
    "print(temperatures.iloc[22, 1])\n",
    "\n",
    "# Use slicing to get the first 5 rows\n",
    "print(temperatures.iloc[0:5])\n",
    "\n",
    "# Use slicing to get columns 3 to 4\n",
    "print(temperatures.iloc[:, 2:4])\n",
    "\n",
    "# Use slicing in both directions at once\n",
    "print(temperatures.iloc[0:5, 2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Add a year column to temperatures\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m temperatures[\u001b[33m'\u001b[39m\u001b[33myear\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mtemperatures\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdt\u001b[49m.year\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Pivot avg_temp_c by country and city vs year\u001b[39;00m\n\u001b[32m      5\u001b[39m temp_by_country_city_vs_year = temperatures.pivot_table(\u001b[33m'\u001b[39m\u001b[33mavg_temp_c\u001b[39m\u001b[33m'\u001b[39m, index=[\u001b[33m'\u001b[39m\u001b[33mcountry\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcity\u001b[39m\u001b[33m'\u001b[39m], columns=\u001b[33m'\u001b[39m\u001b[33myear\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/datacamp/lib/python3.13/site-packages/pandas/core/generic.py:6299\u001b[39m, in \u001b[36mNDFrame.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   6292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   6293\u001b[39m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._internal_names_set\n\u001b[32m   6294\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._metadata\n\u001b[32m   6295\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._accessors\n\u001b[32m   6296\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._info_axis._can_hold_identifiers_and_holds_name(name)\n\u001b[32m   6297\u001b[39m ):\n\u001b[32m   6298\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[32m-> \u001b[39m\u001b[32m6299\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/datacamp/lib/python3.13/site-packages/pandas/core/accessor.py:224\u001b[39m, in \u001b[36mCachedAccessor.__get__\u001b[39m\u001b[34m(self, obj, cls)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._accessor\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m accessor_obj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_accessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[32m    226\u001b[39m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[32m    228\u001b[39m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[32m    229\u001b[39m \u001b[38;5;28mobject\u001b[39m.\u001b[34m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m._name, accessor_obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/datacamp/lib/python3.13/site-packages/pandas/core/indexes/accessors.py:643\u001b[39m, in \u001b[36mCombinedDatetimelikeProperties.__new__\u001b[39m\u001b[34m(cls, data)\u001b[39m\n\u001b[32m    640\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data.dtype, PeriodDtype):\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m PeriodProperties(data, orig)\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan only use .dt accessor with datetimelike values\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "# Add a year column to temperatures\n",
    "temperatures['year'] = temperatures['date'].dt.year\n",
    "\n",
    "# Pivot avg_temp_c by country and city vs year\n",
    "temp_by_country_city_vs_year = temperatures.pivot_table('avg_temp_c', index=['country', 'city'], columns='year')\n",
    "\n",
    "# See the result\n",
    "print(temp_by_country_city_vs_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'temp_by_country_city_vs_year' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Subset for Egypt to India\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtemp_by_country_city_vs_year\u001b[49m.loc[\u001b[33m'\u001b[39m\u001b[33mEgypt\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33mIndia\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Subset for Egypt, Cairo to India, Delhi\u001b[39;00m\n\u001b[32m      5\u001b[39m temp_by_country_city_vs_year.loc[(\u001b[33m'\u001b[39m\u001b[33mEgypt\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mCairo\u001b[39m\u001b[33m'\u001b[39m):(\u001b[33m'\u001b[39m\u001b[33mIndia\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mDelhi\u001b[39m\u001b[33m'\u001b[39m)]\n",
      "\u001b[31mNameError\u001b[39m: name 'temp_by_country_city_vs_year' is not defined"
     ]
    }
   ],
   "source": [
    "# Subset for Egypt to India\n",
    "temp_by_country_city_vs_year.loc['Egypt':'India']\n",
    "\n",
    "# Subset for Egypt, Cairo to India, Delhi\n",
    "temp_by_country_city_vs_year.loc[('Egypt','Cairo'):('India','Delhi')]\n",
    "\n",
    "# # Subset for Egypt, Cairo to India, Delhi, and 2005 to 2010\n",
    "temp_by_country_city_vs_year.loc[('Egypt','Cairo'):('India','Delhi'), 2005:2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'temp_by_country_city_vs_year' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Get the worldwide mean temp by year\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m mean_temp_by_year = \u001b[43mtemp_by_country_city_vs_year\u001b[49m.mean()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Filter for the year that had the highest mean temp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(mean_temp_by_year[mean_temp_by_year == mean_temp_by_year.max()])\n",
      "\u001b[31mNameError\u001b[39m: name 'temp_by_country_city_vs_year' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the worldwide mean temp by year\n",
    "mean_temp_by_year = temp_by_country_city_vs_year.mean()\n",
    "\n",
    "# Filter for the year that had the highest mean temp\n",
    "print(mean_temp_by_year[mean_temp_by_year == mean_temp_by_year.max()])\n",
    "\n",
    "# Get the mean temp by city\n",
    "mean_temp_by_city = temp_by_country_city_vs_year.mean(axis=\"columns\")\n",
    "\n",
    "# Filter for the city that had the lowest mean temp\n",
    "print(mean_temp_by_city[mean_temp_by_city == mean_temp_by_city.min()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datacamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
